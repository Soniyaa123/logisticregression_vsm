# -*- coding: utf-8 -*-
"""Copy of TECH400_W7_Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wftOAkYp4HrEt_7gjy8FWNS4qPgVDJbr
"""

import os
import math
import numpy as np
from collections import defaultdict
import re
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from nltk . corpus import stopwords
from nltk . tokenize import word_tokenize
from nltk . stem import WordNetLemmatizer

# Initialize the stop words and lemmatizer
STOPWORDS = set( stopwords . words ('english') )
LEMMATIZER = WordNetLemmatizer ()

#Load documents
def load_documents(directory):
    documents = []
    file_names = []
    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:
                content = f.read().replace('\n', ' ')
                documents.append(content)
            file_names.append(filename)

    return documents, file_names

queries = ["potato", "salt", "flour"]

# Function to clean and preprocess text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\W+', ' ', text)
    tokens = word_tokenize(text)
    tokens = [LEMMATIZER.lemmatize(token) for token in tokens if token not in STOPWORDS]
    return tokens

# Define the directory containing your text files
directory_path = "documents"

# Load the documents
documents, file_names = load_documents(directory_path)

# Now, documents is defined, so let's use it
cleaned_documents = {filename: clean_text(content) for filename, content in zip(file_names, documents)}

# Vocabulary Creation
vocab = set([word for doc in cleaned_documents.values() for word in doc])
vocab = sorted(vocab)
print("Vocabulary:", vocab)

# Function to calculate TF
def term_frequency(term, document):
  tf = document.count(term) / len(document)
  print (tf)
  return tf

# Function to calculate IDF
def inverse_document_frequency(term, all_documents):
  num_docs_containing_term = sum(1 for doc in all_documents if term in doc)
  return math.log(len(all_documents) / (1 + num_docs_containing_term))

# Compute TF-IDF for a document
def compute_tfidf(document, all_documents, vocab):
  tfidf_vector = []
  for term in vocab:
    tf = term_frequency(term, document)
    idf = inverse_document_frequency(term, all_documents)
    tfidf_vector.append(tf * idf)
  return np.array(tfidf_vector)

# Compute cosine similarity between two vectors
def cosine_similarity(vec1, vec2):
  dot_product = np.dot(vec1, vec2)
  norm_vec1 = np.linalg.norm(vec1)
  norm_vec2 = np.linalg.norm(vec2)
  return dot_product / (norm_vec1 * norm_vec2)

# Calculate TF-IDF vectors for documents and queries
doc_tfidf_vectors = [compute_tfidf(doc, cleaned_documents.values(), vocab) for doc in cleaned_documents.values()]

# Calculate TF-IDF vectors for each query separately
query_tfidf_vectors = [compute_tfidf(clean_text(query), cleaned_documents.values(), vocab) for query in queries]

# Calculate cosine similarities
cosine_similarities = []
for query_vector in query_tfidf_vectors:
    similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]
    cosine_similarities.append(similarities)

# Display the results in ranked order (top 5 only)
for i, query in enumerate(queries):
    print(f"\nRanked Result for '{query}':")
    ranked_docs = sorted(enumerate(cosine_similarities[i]), key=lambda x: x[1], reverse=True)

    # Limiting the results to top 5 and filtering out documents with 0 similarity
    for j, similarity in ranked_docs[:5]:
        if similarity > 0:
            print(f"Document {j+1}: {similarity:.4f}")
        else:
            print(f"Document {j+1}: No similarity found")

relevant_docs_per_query = {
    "potato": {13, 1, 11, 2, 3},
    "salt": {10, 15, 8, 13, 6},
    "flour": {4, 5, 7, 8, 6},
}

def precision_at_k(ranked_docs, relevant_docs, k):
    ranked_docs = ranked_docs[:k]
    num_relevant_in_k = len(set(ranked_docs) & relevant_docs)
    if k == 0:
        return 0
    precision = num_relevant_in_k / k
    return precision

# Evaluate precision at k for each query
ranked_docs_ids_list = []
for i, query in enumerate(queries):
    print(f"\nRanked Result for '{query}':")
    ranked_docs_for_query = sorted(enumerate(cosine_similarities[i]), key=lambda x: x[1], reverse=True)

    # Adjust document IDs by adding 1
    ranked_docs_ids = [(doc_id + 1, similarity) for doc_id, similarity in ranked_docs_for_query]

    ranked_docs_ids_list.append(ranked_docs_ids)
    relevant_docs = relevant_docs_per_query.get(query, set())
    k = 5
    precision_k = precision_at_k(ranked_docs_ids, relevant_docs, k)
    print(f"Precision at {k}: {precision_k:.4f}")

"""Logistic Regression Model"""

import numpy as np

class LogisticRegression:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        # Initialize weights and bias
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        # Gradient descent
        for _ in range(self.epochs):
            # Linear model
            linear_model = np.dot(X, self.weights) + self.bias
            # Apply sigmoid function
            y_predicted = self.sigmoid(linear_model)

            # Compute gradients
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # Update weights and bias
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self.sigmoid(linear_model)
        return [1 if i > 0.5 else 0 for i in y_predicted]


# Create features (X) and target variable (y)
X = []
y = []

# Assuming cosine_similarities and relevant_docs_per_query are available
for i, query in enumerate(queries):
    relevant_docs = relevant_docs_per_query.get(query, set())
    for j, similarity in enumerate(cosine_similarities[i]):
        X.append([similarity])  # Features: cosine similarity
        y.append(1 if j + 1 in relevant_docs else 0)  # Target: relevant or not

X = np.array(X)  # Convert to NumPy array
y = np.array(y)  # Convert to NumPy array

# Training the model
model = LogisticRegression(learning_rate=0.01, epochs=1000)
model.fit(X, y)

y_pred = model.predict(X)

accuracy = accuracy_score(y, y_pred)
# Set average to 'micro', 'macro', 'weighted', or None
precision = precision_score(y, y_pred, average='micro')
recall = recall_score(y, y_pred, average='micro')

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")